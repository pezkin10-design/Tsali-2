#!/usr/bin/env python3
"""
Simplified oemer to TensorFlow Lite converter.

This script handles the practical conversion of oemer models.
If direct model extraction doesn't work, it provides fallback mechanisms.

Usage:
    python convert_oemer_simple.py -o ./tflite_models
"""

import os
import sys
import warnings
import argparse
from pathlib import Path
from typing import Optional
import json

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings('ignore')

# Try importing all required packages
PACKAGES_REQUIRED = {
    'tensorflow': '2.14.0',
    'torch': '2.0+',
    'torchvision': '0.15+',
    'oemer': 'latest',
}

def check_dependencies():
    """Verify all required packages are installed."""
    missing = []
    for package, version in PACKAGES_REQUIRED.items():
        try:
            __import__(package)
        except ImportError:
            missing.append(f"{package}=={version}")
    
    if missing:
        print("‚ùå Missing required packages:")
        for pkg in missing:
            print(f"   pip install {pkg}")
        sys.exit(1)


def convert_models():
    """Convert oemer models to TFLite format."""
    
    import tensorflow as tf
    import torch
    import numpy as np
    from pathlib import Path
    
    print("üéµ oemer ‚Üí TensorFlow Lite Converter")
    print("=" * 60)
    
    # Option 1: Try importing oemer models directly
    try:
        print("üì¶ Loading oemer models...")
        from oemer.models import Staff, Clef
        
        # Load pre-trained models
        staff_model = Staff.load_from_checkpoint(
            checkpoint_path=Staff.download_pretrained_model()
        )
        staff_model.eval()
        
        print("‚úÖ Loaded staff detection model")
        
        # For symbol recognition, load clef model as example
        # (In production, you'd load the full symbol classifier)
        clef_model = Clef.load_from_checkpoint(
            checkpoint_path=Clef.download_pretrained_model()
        )
        clef_model.eval()
        
        print("‚úÖ Loaded symbol recognition model (using Clef as example)")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Note: Direct oemer import encountered: {e}")
        print("   Creating mock TFLite models for demonstration...")
        return create_mock_tflite_models()
    
    # Option 2: Convert PyTorch models to TFLite using ONNX
    try:
        print("\nüîÑ Converting models to ONNX format...")
        
        # Create sample inputs
        staff_input = torch.randn(1, 3, 512, 512)
        symbol_input = torch.randn(1, 3, 128, 128)
        
        # Export to ONNX
        torch.onnx.export(
            staff_model, staff_input,
            "staff_detector.onnx",
            input_names=["input"],
            output_names=["output"],
            opset_version=13,
            verbose=False
        )
        print("‚úÖ Created staff_detector.onnx")
        
        torch.onnx.export(
            clef_model, symbol_input,
            "symbol_recognizer.onnx",
            input_names=["input"],
            output_names=["output"],
            opset_version=13,
            verbose=False
        )
        print("‚úÖ Created symbol_recognizer.onnx")
        
        # Convert ONNX to TFLite
        print("\nüîÑ Converting ONNX to TensorFlow Lite...")
        
        import onnx
        from onnx_tf.backend import prepare
        
        # Staff detector
        onnx_model = onnx.load("staff_detector.onnx")
        tf_rep = prepare(onnx_model)
        tf_rep.export_graph("staff_detector_tf")
        
        converter = tf.lite.TFLiteConverter.from_saved_model("staff_detector_tf")
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]
        
        tflite_model = converter.convert()
        with open("staff_detector.tflite", "wb") as f:
            f.write(tflite_model)
        
        size1 = os.path.getsize("staff_detector.tflite") / (1024 * 1024)
        print(f"‚úÖ Saved staff_detector.tflite ({size1:.2f} MB)")
        
        # Symbol recognizer
        onnx_model = onnx.load("symbol_recognizer.onnx")
        tf_rep = prepare(onnx_model)
        tf_rep.export_graph("symbol_recognizer_tf")
        
        converter = tf.lite.TFLiteConverter.from_saved_model("symbol_recognizer_tf")
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]
        
        tflite_model = converter.convert()
        with open("symbol_recognizer.tflite", "wb") as f:
            f.write(tflite_model)
        
        size2 = os.path.getsize("symbol_recognizer.tflite") / (1024 * 1024)
        print(f"‚úÖ Saved symbol_recognizer.tflite ({size2:.2f} MB)")
        
        total = size1 + size2
        print(f"\nüìä Total size: {total:.2f} MB")
        
        if total <= 50:
            print("‚úÖ SUCCESS: Within 50 MB budget!")
        else:
            print(f"‚ö†Ô∏è  Size ({total:.2f} MB) exceeds target - consider int8 quantization")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Conversion failed: {e}")
        print("\nFallback: Creating mock TFLite models...")
        return create_mock_tflite_models()


def create_mock_tflite_models():
    """
    Create mock TFLite models for testing the pipeline.
    Use these to test the React Native integration while the real models convert.
    """
    import tensorflow as tf
    import numpy as np
    
    print("\nüìù Creating mock TFLite models (for testing)...")
    
    # Create simple models that mimic oemer behavior
    
    # Staff Detector: 512x512 ‚Üí heatmap (512x512)
    staff_input = tf.keras.Input(shape=(512, 512, 3), batch_size=1)
    x = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')(staff_input)
    x = tf.keras.layers.MaxPooling2D(4)(x)
    x = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')(x)
    x = tf.keras.layers.UpSampling2D(4)(x)
    staff_output = tf.keras.layers.Conv2D(1, 1, padding='same', activation='sigmoid')(x)
    staff_model = tf.keras.Model(inputs=staff_input, outputs=staff_output)
    
    # Convert to TFLite
    converter = tf.lite.TFLiteConverter.from_keras_model(staff_model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_types = [tf.float16]
    
    tflite_staff = converter.convert()
    with open("staff_detector.tflite", "wb") as f:
        f.write(tflite_staff)
    
    size1 = os.path.getsize("staff_detector.tflite") / (1024 * 1024)
    print(f"‚úÖ Created staff_detector.tflite ({size1:.2f} MB)")
    
    # Symbol Recognizer: 128x128 ‚Üí class logits (num_classes)
    symbol_input = tf.keras.Input(shape=(128, 128, 3), batch_size=1)
    x = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')(symbol_input)
    x = tf.keras.layers.MaxPooling2D(2)(x)
    x = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    symbol_output = tf.keras.layers.Dense(128, activation='softmax')(x)  # 128 symbol classes
    symbol_model = tf.keras.Model(inputs=symbol_input, outputs=symbol_output)
    
    # Convert to TFLite
    converter = tf.lite.TFLiteConverter.from_keras_model(symbol_model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_types = [tf.float16]
    
    tflite_symbol = converter.convert()
    with open("symbol_recognizer.tflite", "wb") as f:
        f.write(tflite_symbol)
    
    size2 = os.path.getsize("symbol_recognizer.tflite") / (1024 * 1024)
    print(f"‚úÖ Created symbol_recognizer.tflite ({size2:.2f} MB)")
    
    total = size1 + size2
    print(f"\nüìä Mock models created: {total:.2f} MB total")
    print("\n‚ö†Ô∏è  IMPORTANT: These are mock models for testing integration only.")
    print("   For production, run: python convert_oemer_to_tflite.py")
    
    return True


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Convert oemer models to TensorFlow Lite"
    )
    parser.add_argument(
        "-o", "--output",
        default="./tflite_models",
        help="Output directory"
    )
    parser.add_argument(
        "--check-only",
        action="store_true",
        help="Only check dependencies, don't convert"
    )
    parser.add_argument(
        "--mock",
        action="store_true",
        help="Create mock models for testing"
    )
    
    args = parser.parse_args()
    
    # Check dependencies
    print("Checking dependencies...")
    try:
        check_dependencies()
        print("‚úÖ All dependencies available\n")
    except SystemExit:
        if args.check_only:
            sys.exit(1)
        print("‚ö†Ô∏è  Some dependencies missing, will create mock models\n")
    
    if args.check_only:
        sys.exit(0)
    
    # Create output directory
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    os.chdir(output_dir)
    
    # Convert or create mock
    if args.mock:
        success = create_mock_tflite_models()
    else:
        success = convert_models()
    
    # Print next steps
    if success:
        print("\n" + "=" * 60)
        print("üìã NEXT STEPS:")
        print("=" * 60)
        print(f"1. Copy .tflite files from {output_dir} to:")
        print("   sheet-music-scanner/src/assets/models/")
        print("")
        print("2. Update metro.config.js to bundle .tflite files")
        print("")
        print("3. Install react-native-fast-tflite:")
        print("   cd sheet-music-scanner")
        print("   npm install react-native-fast-tflite")
        print("")
        print("4. Create OMRService.ts to use TFLite models locally")
        print("=" * 60)
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
